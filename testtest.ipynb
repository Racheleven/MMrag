{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=\"sk-vzueoszaobqmmtpcbxtycvombxzwbxrbmtzllrdnahoebsui\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"你是一个意图识别专家。只有两种情况：1.历史记录里已经包含这个问题的答案-> 输出 answer；2.这个问题不能直接根据上下文回答->输出 search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message=[{\"role\": \"user\", \"content\": \"你好，世界！\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='莉莉是一个七年级的中国女孩'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='你想知道更多关于莉莉的事情？'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='对呀，你知道莉莉大概几岁吗'), additional_kwargs={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='莉莉大概13岁，因为七年级的学生通常在13岁左右。如果你有更多关于莉莉的具体问题或想了解她的兴趣爱好、性格特点或最近发生的事情，可以告诉我，我会尽力提供帮助！', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 216, 'prompt_tokens': 34, 'total_tokens': 250, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'system_fingerprint': '', 'id': '01962aa0e17f75354735813cfcd376e9', 'finish_reason': 'stop', 'logprobs': None}, id='run-0eb99369-41b1-4d51-80d1-a0278b58fc62-0', usage_metadata={'input_tokens': 34, 'output_tokens': 216, 'total_tokens': 250, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"莉莉是一个七年级的中国女孩\"),\n",
    "        (\"assistant\", \"你想知道更多关于莉莉的事情？\"),\n",
    "        (\"user\", \"对呀，你知道莉莉大概几岁吗\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm\n",
    "print(prompt)\n",
    "msg=[\n",
    "        # (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"莉莉是一个七年级的中国女孩\"),\n",
    "        (\"assistant\", \"你想知道更多关于莉莉的事情？\"),\n",
    "        (\"user\", \"对呀，你知道莉莉大概几岁吗\"),]\n",
    "llm.invoke(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=f\"\"\"你是一个意图识别专家，处理以下三种情况：\n",
    "\n",
    "1. 如果历史记录中已包含该查询的答案，输出：{\"router\": \"answer\", \"query\": \"\"}。\n",
    "2. 如果该查询不能直接根据历史记录回答，输出：{\"router\": \"search\", \"query\": \"\"}。\n",
    "3. 如果该查询不能直接回答且内容模糊不清，需要根据历史记录内容对查询进行改写，使其更加清晰和明确，输出：{\"router\": \"search\", \"query\": \"改写后的查询\"}。\n",
    "\n",
    "请根据以下历史记录进行处理：\n",
    "{{context}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=\"sk-vzueoszaobqmmtpcbxtycvombxzwbxrbmtzllrdnahoebsui\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "你是一个意图识别专家，处理以下三种情况：\n",
      "\n",
      "1. 如果历史记录中已包含能够回答该查询的信息，输出：{\"router\": \"answer\", \"query\": \"\"}。\n",
      "2. 如果该查询不能直接根据历史记录回答，输出：{\"router\": \"search\", \"query\": \"\"}。\n",
      "3. 如果该查询不能直接回答且内容模糊不清，需要根据历史记录内容对查询进行改写，使其更加清晰和明确，输出：{\"router\": \"search\", \"query\": \"改写后的查询\"}。\n",
      "\n",
      "查询：\n",
      "莉莉是男的女的\n",
      "\n",
      "请根据以下历史记录进行处理：\n",
      "user: 莉莉是一个七年级的中国女孩\n",
      "assistant: 你想知道更多关于莉莉的事情？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "你是一个意图识别专家，处理以下三种情况：\n",
    "\n",
    "1. 如果历史记录中已包含能够回答该查询的信息，输出：{{\"router\": \"answer\", \"query\": \"\"}}。\n",
    "2. 如果该查询不能直接根据历史记录回答，输出：{{\"router\": \"search\", \"query\": \"\"}}。\n",
    "3. 如果该查询不能直接回答且内容模糊不清，需要根据历史记录内容对查询进行改写，使其更加清晰和明确，输出：{{\"router\": \"search\", \"query\": \"改写后的查询\"}}。\n",
    "\n",
    "查询：\n",
    "{query}\n",
    "\n",
    "请根据以下历史记录进行处理：\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "msg=[(\"user\", \"莉莉是一个七年级的中国女孩\"),\n",
    "(\"assistant\", \"你想知道更多关于莉莉的事情？\"),]\n",
    "\n",
    "# 将历史记录转换为字符串，格式为对话文本\n",
    "context = \"\\n\".join([f\"{role}: {message}\" for role, message in msg])\n",
    "\n",
    "# 使用 format 方法来动态插入历史记录\n",
    "formatted_prompt = system_prompt.format(query=\"莉莉是男的女的\",context=context)\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg=llm.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON data: {'router': 'answer', 'query': ''}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.output_parsers.structured import StructuredOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# 提取 JSON 内容\n",
    "json_content = ai_msg.content.strip(\"```json\\n\").strip(\"```\")\n",
    "\n",
    "# 解析 JSON 数据\n",
    "try:\n",
    "    parsed_data = json.loads(json_content)\n",
    "    print(\"Parsed JSON data:\", parsed_data)\n",
    "    \n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"Error parsing JSON:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON data: {'evidence_list': ['比亚迪全年销售302万辆，同比增长62.3%，市场份额达到31.7%。', '特斯拉销售180万辆，同比增长38.6%。']}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=\"sk-vzueoszaobqmmtpcbxtycvombxzwbxrbmtzllrdnahoebsui\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "evidence=\"\"\"\n",
    "2023年新能源汽车销量报告显示，比亚迪全年销售302万辆，同比增长62.3%，市场份额达到31.7%。特斯拉销售180万辆，同比增长38.6%。理想汽车销售37.6万辆，同比增长182.2%。\n",
    "根据中国汽车工业协会数据，2023年中国新能源汽车渗透率达35.8%，较2022年提升8.2个百分点。其中纯电动车占比72%，插电混动占比28%。\n",
    "充电基础设施方面，截至2023年底全国充电桩总量达859.6万台，车桩比降至2.5:1。特来电、星星充电和国家电网占据市场份额前三。\n",
    "\"\"\"\n",
    "query=\"2023年比亚迪和特斯拉的销量及增长情况如何？\"\n",
    "\n",
    "def extract_evidence(evidence, query):\n",
    "    prompt = f\"\"\"你是一个专业的证据抽取助手。请严格根据提供的资料内容，提取能直接回答查询问题的证据片段，并按照指定格式返回。\n",
    "    \n",
    "    资料内容：\n",
    "    {evidence}\n",
    "    \n",
    "    查询问题：\n",
    "    {query}\n",
    "    \n",
    "    提取要求：\n",
    "    1. 只输出与查询直接相关的证据\n",
    "    2. 保持原文表述，不要修改或总结\n",
    "    3. 以JSON格式返回结果，包含一个名为\"evidence_list\"的数组\n",
    "    4. 不要添加任何解释或说明\n",
    "    \n",
    "    正确格式示例：\n",
    "    {{\n",
    "        \"evidence_list\": [\n",
    "            \"证据片段1\",\n",
    "            \"证据片段2\"\n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # 提取 JSON 内容\n",
    "    json_content = response.content.strip(\"```json\\n\").strip(\"```\")\n",
    "\n",
    "    try:\n",
    "        parsed_data = json.loads(json_content)\n",
    "        print(\"Parsed JSON data:\", parsed_data)\n",
    "        \n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON:\", e)\n",
    "# 使用示例\n",
    "extract_evidence(evidence, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 只返回能回答问题的事实陈述\n",
    "#     2. 可以适当整理语言，但不要添加额外信息"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
